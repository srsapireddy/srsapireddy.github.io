---
title: "Machine Learning Project: Support Vector Regressor"
date: 2020-02-08
tags: [Machine learning, data science, Multiple Linear]
header:
    image: "/images/svr/svr.png"
excerpt: "Machine Learning, Regression, Data Science"
---

# Support Vector Regressor â€”
In SVM for classification problem we actually try to separate the class as far as possible from the separating line (Hyperplane) and unlike logistic regression, we create a safety boundary from both sides of the hyperplane (different between logistic regression and SVM classification is in their loss function). Eventually, having a separated different data points as far as possible from hyperplane.<br>

In SVM for regression problem, We want to fit a model to predict a quantity for future. Therefore, we want the data point(observation) to be as close as possible to the hyperplane unlike SVM for classification. The SVM regression inherited from Simple Regression like (Ordinary Least Square) by this difference that we define an epsilon range from both sides of hyperplane to make the regression function insensitive to the error unlike SVM for classification that we define a boundary to be safe for making the future decision(prediction). Eventually, SVM in Regression has a boundary like SVM in classification but the boundary for Regression is for making the regression function insensitive respect to the error but the boundary for classification is only to be way far from hyperplane(decision boundary) to distinguish between class for future (that is why we call it safety margin).


Link to my GitHub page [Support_Vector_Regressor](https://github.com/srsapireddy/Machine-Learning-Files-in-Python-and-R/tree/master/Regression/5.%20Support%20Vector%20Regression%20(SVR))
