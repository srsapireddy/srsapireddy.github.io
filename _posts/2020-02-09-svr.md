---
title: "Machine Learning Project: Support Vector Regressor"
date: 2020-02-08
tags: [Machine learning, data science, Multiple Linear]
header:
    image: "/images/svr/svr.png"
excerpt: "Machine Learning, Regression, Data Science"
---

# Support Vector Regressor â€”
In SVM for classification problem we actually try to separate the class as far as possible from the separating line (Hyperplane) and unlike logistic regression, we create a safety boundary from both sides of the hyperplane (different between logistic regression and SVM classification is in their loss function). Eventually, having a separated different data points as far as possible from hyperplane.<br>

In SVM for regression problem, We want to fit a model to predict a quantity for future. Therefore, we want the data point(observation) to be as close as possible to the hyperplane unlike SVM for classification. The SVM regression inherited from Simple Regression like (Ordinary Least Square) by this difference that we define an epsilon range from both sides of hyperplane to make the regression function insensitive to the error unlike SVM for classification that we define a boundary to be safe for making the future decision(prediction). Eventually, SVM in Regression has a boundary like SVM in classification but the boundary for Regression is for making the regression function insensitive respect to the error but the boundary for classification is only to be way far from hyperplane(decision boundary) to distinguish between class for future (that is why we call it safety margin).


Link to my GitHub page [Support_Vector_Regressor](https://github.com/srsapireddy/Machine-Learning-Files-in-Python-and-R/tree/master/Regression/5.%20Support%20Vector%20Regression%20(SVR))

R code block:
```r
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
# library(caTools)
# set.seed(123)
# split = sample.split(dataset$DependentVariable, SplitRatio = 0.8)
# training_set = subset(dataset, split == TRUE)
# test_set = subset(dataset, split == FALSE)

# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)

# Fitting the SVR to the dataset
install.packages('e1071')
library(e1071)
regressor = svm(formula = Salary ~ ., data = dataset, type = 'eps-regression')

# Predicting a new Result
# Adding polynomial regression terms -> Level1, Level2, Level3 and Level4
y_pred = predict(regressor, data.frame(Level = 6.5))

# Visualizing the SVR Results
install.packages('ggplot2')
library(ggplot2)
ggplot() + geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
  geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset)), colour = 'blue') +
  ggtitle('Truth or Bluff (SVR)') +
  xlab('Level') +
  ylab('Salaries')
```

Output:
<img src="{{ site.url }}{{ site.baseurl }}/images/svr/svr3.JPG" alt="linearly separable data">
